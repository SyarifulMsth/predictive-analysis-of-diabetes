# -*- coding: utf-8 -*-
"""Diabetes Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1txjcGwVmu1tCjABNaSxLNRFYzBofPbD3

# Diabetes Prediction

## Business Understanding

Building machine learning models for predictive analytics

## Data Understanding

In this project the dataset used is [Diabetes Dataset](https://www.kaggle.com/datasets/mathchi/diabetes-data-set). The dataset consists of 768 data records with 9 features. Following are the details of the dataset:


The following is a table containing details of the Diabetes Dataset dataset:


| Feature                    | Description                                                            |
|----------------------------|------------------------------------------------------------------------|
| Pregnancies                | Number of times pregnant                                               |
| Glucose                    | Plasma glucose concentration a 2 hours in an oral glucose tolerance test|
| BloodPressure              | Diastolic blood pressure (mm Hg)                                       |
| SkinThickness              | Triceps skin fold thickness (mm)                                       |
| Insulin                    | 2-Hour serum insulin (mu U/ml)                                         |
| BMI                        | Body mass index (weight in kg/(height in m)^2)                         |
| DiabetesPedigreeFunction   | Diabetes pedigree function                                             |
| Age                        | Age in years                                                           |
| Outcome                    | Class variable (0 or 1)                                                |

### Gathering Data
"""

# import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# ignore warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# loading dataset using read_csv
df = pd.read_csv('/content/diabetes.csv')
df.head()

"""### Data Assesing and Data Cleaning

Data Assessing is the process of identifying and understanding existing data, including exploring the structure, quality and usefulness of the data. This involves identifying problems such as missing, duplicate, or invalid data, as well as evaluating the suitability of the data for analytical purposes or other uses.

Data Cleaning is the process of addressing problems discovered during data assessment, such as filling in missing values, removing duplicates, or correcting incorrect formatting. The goal is to ensure the data is clean, consistent, and ready to be used in further analysis or modeling.
"""

# Returns the number of rows and columns of the DataFrame
df.shape

# Displays information about the DataFrame, including the number of non-null values, data type, and memory usage
df.info()

# Counts the number of missing values ​​(NaN) in each DataFrame column
df.isna().sum()

# Counts the number of rows that are duplicates in a DataFrame
df.duplicated().sum()

# Displays summary statistics of the DataFrame
df.describe()

"""Based on the data assessing and data cleaning processes that have been carried out, in the dataset used there are no missing values ​​or duplicated data. Then, by using the *info()* function, we can identify the features contained in the dataset, which include Pregnancies, Glucose, Blood Pressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, and Outcome. Apart from that, using the *describe()* function can provide a statistical summary of the dataset used, such as mean, standard deviation, min max value, and so on.

## Exploratory Data Analysis (EDA)

Exploratory data analysis (EDA) is the process of initial investigation into data to analyze characteristics, find patterns, anomalies, and check assumptions in the data. This technique usually uses statistical assistance and graphical representation or visualization.

In the code below we find out how the number of people with diabetes and non-diabetes compares.
"""

import matplotlib.pyplot as plt

# Pisahkan data berdasarkan nilai Outcome
diabetes = df[df['Outcome'] == 1]
non_diabetes = df[df['Outcome'] == 0]

# Hitung jumlah data untuk masing-masing kelompok
num_diabetes = len(diabetes)
num_non_diabetes = len(non_diabetes)

# Buat pie chart
labels = f'Diabetes : {num_diabetes} orang', f'Non-Diabetes : {num_non_diabetes} orang'
sizes = [num_diabetes, num_non_diabetes]
colors = ['#EF476F', '#06D6A0']  # Gunakan warna hijau (#06D6A0) dan merah (#EF476F)
explode = (0.1, 0)  # Pisahkan potongan "Diabetes"

plt.figure(figsize=(8, 6))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Pastikan pie chart berbentuk lingkaran
plt.title('Diabetes & Non-Diabetes')
plt.show()

"""Creates a histogram for each numeric column in the DataFrame"""

df.hist(bins=50, figsize=(20, 15), color='#06D6A0', edgecolor='black')
plt.show()

"""The code below aims to calculate the correlation between the numeric columns in the df DataFrame and then sort them based on the correlation with the 'Outcome' column.

If the correlation coefficient is close to +1, it indicates a positive relationship between two variables. This means that when one variable rises, it is likely that other variables also rise. Conversely, if the correlation coefficient is close to -1, it indicates a negative relationship between two variables. This means that when one variable rises, the other variable tends to fall.

If the correlation coefficient is close to 0, it indicates there is no linear relationship between two variables. However, it is important to remember that the absence of a linear correlation does not mean there is no relationship at all; it just means that the relationship cannot be explained in the same way as in a positive or negative correlation.
"""

corr_matrix = df.corr()
corr_matrix['Outcome'].sort_values(ascending=False)

df.shape

"""In this project we will detect outliers using boxplot data visualization techniques. Then, these outliers will be handled using the IQR (Inter Quartile Range) method."""

def box_plots_all_columns(df):
    num_cols = len(df.columns)
    num_rows = (num_cols + 3) // 4
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))
    plt.suptitle("Box Plot before median imputation")

    for i, column in enumerate(df.columns):
        row = i // 4
        col = i % 4
        sns.boxplot(df[column], ax=axes[row, col], color='#EF476F')
        axes[row, col].set_title(f"Box Plot - {column}")

    for i in range(num_cols, num_rows * 4):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()
    plt.show()

box_plots_all_columns(df)

"""It can be seen from the data visualization above that there are outliers in the dataset used. So it is necessary to handle outlier data.

IQR tells us the variation in the data set.Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR treated as outliers.

* Q1 represents the 1st quartile/25th percentile of the data.
* Q2 represents the 2nd quartile/median/50th percentile of the data.
* Q3 represents the 3rd quartile/75th percentile of the data.
* (Q1–1.5*IQR) represent the smallest value in the data set and (Q3+1.5*IQR) represnt the largest value in the data set.
"""

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR=Q3-Q1
df=df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]

df.shape

"""Displays data visualization results after applying the IQR technique."""

def box_plots_all_columns(df):
    num_cols = len(df.columns)
    num_rows = (num_cols + 3) // 4
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))
    plt.suptitle("Box Plot after median imputation")

    for i, column in enumerate(df.columns):
        row = i // 4
        col = i % 4
        sns.boxplot(df[column], ax=axes[row, col], color='#06D6A0')
        axes[row, col].set_title(f"Box Plot - {column}")

    for i in range(num_cols, num_rows * 4):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()
    plt.show()

box_plots_all_columns(df)

"""Based on the previous results of comparing the number of people with diabetes and non-diabetes, it can be seen that the number of data for people with diabetes is only 34.9% and non-diabetes is 65.1%. This indicates that the data is not balanced (imbalance data). So it is necessary to handle imbalanced data, because unbalanced data can result in bias in the model and accuracy results can be inaccurate.

There are two methods for dealing with imbalance data, namely oversampling and undersampling, depending on the case and dataset you have.

Oversampling: Used when we have a small dataset and want to sample more minority classes. Oversampling with SMOTE can help improve model accuracy because it does not lose data, but it can increase the risk of overfitting if not managed properly.

Undersampling: Used when we have a large dataset and want to reduce the number of majority class samples. Undersampling can help reduce training time and improve class balance, but it can reduce useful information if majority class samples are randomly removed.

In this case, we will apply the oversamling method because the dataset used is in the small category, so using this method can make the dataset balanced.
"""

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from collections import Counter

# Split data
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Oversampling using SMOTE
oversample = SMOTE()
X_over, y_over = oversample.fit_resample(X, y)
counter_over = Counter(y_over)

print("Setelah oversampling:", counter_over)

plt.figure(figsize=(6, 3))
plt.subplot(1, 2, 1)
colors = ['#06D6A0', '#EF476F']  # Gunakan warna merah dan hijau
plt.bar(counter_over.keys(), counter_over.values(), color=colors)
plt.xticks([0, 1])
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Oversampling Result')
plt.ylim([0, max(counter_over.values()) + 100])

for i, v in enumerate(counter_over.values()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""Imbalanced data has been successfully handled, the data is balanced and can be used for the model development stage.

## Data splitting

The next stage is dividing the dataset into train data and test data. Train data will be used in the model training process, and test data will be used to test or find out how well the model that has been created generalizes to new data that it has never seen.

After conducting several experiments, in this case the dataset will be divided into a 70:30 proportion, namely train data (70%) and test data (30%).
"""

from sklearn.model_selection import train_test_split

X = df.drop('Outcome', axis=1)
y = df.Outcome

X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=42)

"""## Standardization

Before developing a machine learning model, a standardization process is first carried out. This stage is used to process the numerical features in the data so that it has a mean of 0 and a standard deviation of 1. One of the purposes of standardization is because many machine learning algorithms perform better or are stable when the numerical features are on the same scale. With standardization, these features are treated uniformly, which can improve model performance.
"""

# standardization using StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([('std_scaler', StandardScaler())])
X_train_prepared = num_pipeline.fit_transform(X_train)
X_test_prepared = num_pipeline.transform(X_test)

"""## Modelling & Evaluate"""

def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")

    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))
        print("Test Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")

"""### Logistic Regression"""

logistic_regression__model = LogisticRegression(max_iter=1000)
logistic_regression__model.fit(X_train, y_train)

print_score(logistic_regression__model, X_train, y_train, X_test, y_test, train=True)
print_score(logistic_regression__model, X_train, y_train, X_test, y_test, train=False)

"""### K-Nearest Neighbors (KNN)"""

knn__model = KNeighborsClassifier(n_neighbors=11, p=2, metric='euclidean')
knn__model.fit(X_train, y_train)

print_score(knn__model, X_train, y_train, X_test, y_test, train=True)
print_score(knn__model, X_train, y_train, X_test, y_test, train=False)

"""### Decision Tree Classifier"""

decision_tree__model = DecisionTreeClassifier(random_state=42)
decision_tree__model.fit(X_train, y_train)

print_score(decision_tree__model, X_train, y_train, X_test, y_test, train=True)
print_score(decision_tree__model, X_train, y_train, X_test, y_test, train=False)

"""### Random Forest"""

random_forest__model = RandomForestClassifier(n_estimators=100)
random_forest__model.fit(X_train, y_train)

print_score(random_forest__model, X_train, y_train, X_test, y_test, train=True)
print_score(random_forest__model, X_train, y_train, X_test, y_test, train=False)

# Initialize models
models = {
    "Logistic Regression \t": LogisticRegression(max_iter=1000),
    "KNN \t\t\t": KNeighborsClassifier(n_neighbors=11, p=2, metric='euclidean'),
    "Decision Tree \t\t": DecisionTreeClassifier(random_state=42),
    "Random Forest \t\t": RandomForestClassifier(n_estimators=100)
}

# Train and evaluate each model
results_train = {}
results_test = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    acc_score_train = accuracy_score(y_train, y_pred_train)
    results_train[name] = acc_score_train

    y_pred_test = model.predict(X_test)
    acc_score_test = accuracy_score(y_test, y_pred_test)
    results_test[name] = acc_score_test

# Print results
print("\n======================")
print("Train Accuracy Scores:")
print("======================")
for name, score in results_train.items():
    print(f"{name}: {score:.4f}")

print("\n======================")
print("Test Accuracy Scores:")
print("======================")
for name, score in results_test.items():
    print(f"{name}: {score:.4f}")

"""## Best model

Based on the results of model training, it can be seen that the best model is Random Forest Classification with Train Accuracy 100% and Test Accuracy 82%. With a classification report as follows:
"""

print_score(random_forest__model, X_train, y_train, X_test, y_test, train=True)
print_score(random_forest__model, X_train, y_train, X_test, y_test, train=False)

"""--- End of Code ---"""